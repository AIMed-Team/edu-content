{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfsqFK384QI_"
   },
   "source": [
    "In this notebook, you should implement an exciting task, write a caption for images with an intelligent agent! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EedZ5qvW4QJF"
   },
   "source": [
    "We use the [COCO dataset](https://cocodataset.org/) for this purpose. COCO is large-scale object detection, segmentation, and captioning dataset. Also, we use the pycocotools library for some data-related works. So, you should install it first. Maybe it needs some dependencies that you have not on your PC. So, we recommend running this notebook on Google collab. You should upload data_related.py in the content folder on Colab if you want to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-bM-Bt8F4QJH",
    "outputId": "4fcaef9c-11ff-4710-fd10-4432c6258638"
   },
   "outputs": [],
   "source": [
    "!pip install pycocotools\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sFyODDm-4QJK",
    "outputId": "b0b396e7-be3c-44d4-cce5-4d389fceeeb4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('opt' , exist_ok=True)\n",
    "os.chdir( 'opt' )\n",
    "!git clone 'https://github.com/cocodataset/cocoapi.git'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command imports some data-related functions, and it takes about 10 minutes for running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v10C2fSh4QJL",
    "outputId": "957a9206-6631-42d0-86d8-247e42a533e9"
   },
   "outputs": [],
   "source": [
    "import data_related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhP_EYZ-4QJM"
   },
   "source": [
    "Your network should have two parts, a CNN for understanding the image and an LSTM for generating related sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOPIiotM4QJN"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2P5NMaB14QJP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46wOlClL4QJQ"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        # todo: Define a CNN with an extended fully-connected. Your output should be of the shape Batch_Size x embed_size.\n",
    "        # Make sure that your model is strong enough to encode the image properly.\n",
    "        #######################\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        \n",
    "        #######################\n",
    "    \n",
    "    def forward(self, images):\n",
    "        features = None\n",
    "        #######################\n",
    "        features = self.resnet(images)        \n",
    "        features = features.view(features.size(0), -1)        \n",
    "        features = self.embed(features)  \n",
    "\n",
    "        #######################\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJYsAFRh4QJR"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size \n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        # todo: Define an embedding layer to transform inputs from \"vocab_size\" dim to \"embed size\" dim.\n",
    "        #######################\n",
    "        self.word_embedding = nn.Embedding( self.vocab_size , self.embed_size )\n",
    "        \n",
    "        \n",
    "        #######################\n",
    "        \n",
    "        # todo: Define an LSTM decoder with input size, hidden size, and num layers specified in the input.  \n",
    "        #######################\n",
    "        self.lstm  = nn.LSTM(    input_size  =  self.embed_size , \n",
    "                             hidden_size = self.hidden_size,\n",
    "                             num_layers  = self.num_layers ,\n",
    "                             batch_first = True \n",
    "                             )\n",
    "        \n",
    "        #######################\n",
    "        \n",
    "        # todo: Define a fully-connected layer to transform the output hidden size of LSTM to a \"vocab_size\" dim vector.\n",
    "        #######################\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear( self.hidden_size , self.vocab_size  )\n",
    "        #######################\n",
    "    def init_hidden(self, batch_size):\n",
    "        return ( torch.zeros( self.num_layers , batch_size , self.hidden_size  ).to(device),\n",
    "        torch.zeros( self.num_layers , batch_size , self.hidden_size  ).to(device) )\n",
    "    \n",
    "    def forward(self, features, captions):            \n",
    "        captions = captions[:,  :-1]      \n",
    "        self.batch_size = features.shape[0]\n",
    "        self.hidden = self.init_hidden( self.batch_size )\n",
    "        outputs = None  \n",
    "        \n",
    "        # todo: Compute the output of the model.\n",
    "        #######################\n",
    "        embeds = self.word_embedding( captions )\n",
    "        inputs = torch.cat( ( features.unsqueeze(dim=1) , embeds ) , dim =1  )      \n",
    "        lstm_out , self.hidden = self.lstm(inputs , self.hidden)      \n",
    "        outputs = self.fc( lstm_out )\n",
    "        \n",
    "        \n",
    "        #######################\n",
    "        return outputs\n",
    "\n",
    "    def generate(self, inputs, max_len=20):\n",
    "        final_output = []\n",
    "        batch_size = inputs.shape[0]         \n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        max_sent_length = 20\n",
    "    \n",
    "        # todo: You should pass hidden state and previous vocab to LSTM successively, and stop generating when\n",
    "        # The length of the sentence exceeds max_sent_length, or EOS token (end of sentence, index 1) occurs.\n",
    "        # Just return indexes in final_output.\n",
    "        #######################\n",
    "        while True:\n",
    "            lstm_out, hidden = self.lstm(inputs, hidden) \n",
    "            outputs = self.fc(lstm_out)  \n",
    "            outputs = outputs.squeeze(1) \n",
    "            _, max_idx = torch.max(outputs, dim=1) \n",
    "            final_output.append(max_idx.cpu().numpy()[0].item())             \n",
    "            if (max_idx == 1 or len(final_output) >=20 ):\n",
    "                break\n",
    "            \n",
    "            inputs = self.word_embedding(max_idx) \n",
    "            inputs = inputs.unsqueeze(1)  \n",
    "        \n",
    "        \n",
    "        \n",
    "        #######################\n",
    "        return final_output  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMh14sUX4QJS"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtTLR4o84QJS"
   },
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 100\n",
    "num_layers =1 \n",
    "num_epochs = 4\n",
    "print_every = 150\n",
    "save_every = 1\n",
    "vocab_size = len(data_related.data_loader_train.dataset.vocab)\n",
    "total_step = math.ceil(len(data_related.data_loader_train.dataset.caption_lengths) / \n",
    "                       data_related.data_loader_train.batch_sampler.batch_size)\n",
    "lr = 0.001\n",
    "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_save_path = 'model_weights/'\n",
    "os.makedirs( model_save_path , exist_ok=True)\n",
    "\n",
    "encoder = Encoder(embed_size)\n",
    "decoder = Decoder(embed_size, hidden_size, vocab_size ,num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkGzXXn-0bpi"
   },
   "outputs": [],
   "source": [
    "# todo: Define loss function and optimizer for encoder and decoder weights.\n",
    "#######################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "all_params = list(decoder.parameters())  + list( encoder.embed.parameters() )\n",
    "optimizer = torch.optim.Adam( params  = all_params , lr = lr  )\n",
    "#######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process may take up to 10 hours. Save the model frequently. If the training process stops for some reason, continue from the last saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MetWL4204QJS",
    "outputId": "f2c9deb1-ab26-453c-a703-a72b9cea530f"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "for e in range(1, num_epochs):\n",
    "    for step in range(total_step):\n",
    "        indices = data_related.data_loader_train.dataset.get_train_indices()\n",
    "        new_sampler = data_related.data.sampler.SubsetRandomSampler(indices)\n",
    "        data_related.data_loader_train.batch_sampler.sampler = new_sampler    \n",
    "        images,captions = next(iter(data_related.data_loader_train))    \n",
    "        images , captions = images.to(device) , captions.to(device)\n",
    "        encoder , decoder = encoder.to(device) , decoder.to(device)\n",
    "        encoder.zero_grad()    \n",
    "        decoder.zero_grad()\n",
    "        # todo: Compute output and loss.\n",
    "        #######################\n",
    "        features = encoder(images)\n",
    "        output = decoder( features , captions )    \n",
    "        loss = criterion( output.view(-1, vocab_size) , captions.view(-1))\n",
    "        #######################\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        stat_vals = 'Epochs [%d/%d] Step [%d/%d] Loss [%.4f] ' %(e+1, num_epochs, step, total_step,loss.item())\n",
    "        if step % print_every == 0:\n",
    "            print(stat_vals)\n",
    "            sys.stdout.flush()\n",
    "            torch.save( encoder.state_dict() ,  os.path.join( model_save_path , 'encoderdata_{}.pkl'.format(e+1) ) )\n",
    "            torch.save( decoder.state_dict() ,  os.path.join( model_save_path , 'decoderdata_{}.pkl'.format(e+1) ) )\n",
    "    if e % save_every == 0:\n",
    "        torch.save( encoder.state_dict() ,  os.path.join( model_save_path , 'encoderdata_{}.pkl'.format(e+1) ) )\n",
    "        torch.save( decoder.state_dict() ,  os.path.join( model_save_path , 'decoderdata_{}.pkl'.format(e+1) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqPh-8am4QJT"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "tu4_-lLu4QJT",
    "outputId": "5458dc25-10b6-4217-d3e5-19c7314fae21"
   },
   "outputs": [],
   "source": [
    "encoder.to(device) \n",
    "decoder.to(device)\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "original_img , processed_img  = next( data_related.data_iter )\n",
    "\n",
    "features  = encoder(processed_img.to(device)).unsqueeze(1)\n",
    "final_output = decoder.generate( features  , max_len=20)\n",
    "data_related.get_sentences(original_img, final_output)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "index.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
