---
layout: content
metadata: notebooks_basics_theoretical_basis_metadata
colab: https://colab.research.google.com/github/AIMed-Team/edu-content/blob/master/notebooks/basics/theoretical_basis/index.ipynb
---

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This notebook contains a roadmap for learning theoretical basis of main ML modules and approaches, and some excercises for evaluating your self. Hope to enjoy and learn :)</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Sources:</p>
<ul>
<li><a href="https://drive.google.com/drive/folders/1hPYtFidk0rUQ9dLuWZjTf7YfNEzvP8MS?usp=sharing">SPML course in Fall 2021</a></li>
<li><a href="https://drive.google.com/drive/folders/1bwxmL1xBK7WD8-GPpKQGzW4GJnamy65W?usp=sharing">AI-Med internship videos</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU">Andrew Ng's ML course</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ">Stanford Natural Language Processing with Deep Learning course | Winter 2021</a></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First of all, please see the starting four sessions of famous <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU">Andrew Ng's ML course</a>. Then, try to answer the following questions.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Linear-Regression">Linear Regression<a class="anchor-link" href="#Linear-Regression">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider the linear regression $\hat{y}=w^T x$ on $S=\{(x^{(i)}, y^{(i)})\}_{i=1}^m$ with loss function $J(w)=\sum_{i=1}^{m} (y^{(i)}-\hat{y}^{(i)})^2$.</p>
<ol>
<li>Simplify $\underset{w}{argmin}$ $J(w)$ by setting derivative of $J(w)$ w.r.t. $w$ to 0.</li>
<li>When the formula of the previous part does not work? Simplify $\underset{w}{argmin}$ $J(w)+\lambda\lVert w \rVert^2$ and describe how this new formulat solves the problem.</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Answer the following questions:</p>
<ol>
<li><p>Prove that
$$\frac{d\sigma(a)}{da}=\sigma(a)(1-\sigma(a)).$$</p>
</li>
<li><p>In logistic regression, we have $p(C_1|x)=\sigma(w^T x)$. Compute the negative log-likelihood for dataset $\{(x^{(1)}, y^{(1)}), ..., (x^{(n)}, y^{(n)})\}$.</p>
</li>
<li><p>Show that by computing gradient of the previous part w.r.t. $w$, we have $\sum_{i=1}^{n}(y^{(i)}-\hat{y}^{(i)})x^{(i)}$. compare this with MSE regression gradients.</p>
</li>
<li><p>show that $$\log{\frac{p(C_1|x)}{p(C_0|x)}}=w_1^T x+w_1'$$. Generalize it to $k$ classes and see the Softmax formula.</p>
</li>
<li><p>(optional) if $$L=-\sum_i y_i\log{p_i}$$ where $p_i=p(C_i|x)$, show that $\nabla_O L(x)=y-p$, where $y$ is label one-hot vector for $x$, $p$ is the output of softmax where $p_i=p(C_i|x)$, and $$o_i=w_i^T x + w_i', 1\leq i\leq k.$$</p>
</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>(optional) In logistic regression for $K$ classes, the posterior probability is computed in the following way
$$\begin{equation}
  \left\{
  \begin{array}{@{}ll@{}}
    P(Y=k|X=x)=\frac{exp(w_k^T x)}{1+\sum_{l=1}^{K-1}exp(w_l^T x)}, &amp; (k=1, ..., K-1) \\
    P(Y=K|X=x)=\frac{1}{1+\sum_{l=1}^{K-1}exp(w_l^T x)}
  \end{array}\right.
\end{equation}$$
For simplicity, consider $w_K=0$.</p>
<ol>
<li><p>How many parameters should be estimated. What are them?</p>
</li>
<li><p>Simplify the following log-likelihood for $n$ training samples $\{(x_1, y_1), ..., (x_n, y_n)\}$
$$L(w_1, ..., w_{K-1})=\sum_{i=1}^{n}\ln{P(Y=y_i|X=x_i)}$$</p>
</li>
<li><p>Compute and simplify the gradient of $L$ w.r.t. each of $w_k$s.</p>
</li>
<li><p>Consider the following objective function. Compute the gradient of $f$ w.r.t. each of $w_k$s.</p>
</li>
</ol>
$$f(w_1, ..., w_{K-1})=L(w_1, ..., w_{K-1})-\frac{\lambda}{2}\sum_{l=1}^{K-1}\lVert w_l\rVert_2^2$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Backpropagation">Backpropagation<a class="anchor-link" href="#Backpropagation">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The source for this topic is:</p>
<ul>
<li>SPML course in Fall 2021 <ul>
<li>session 3 from min 100</li>
<li>session 4</li>
<li>session 5 upto min 45</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Questions">Questions<a class="anchor-link" href="#Questions">¶</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider the following network:
$$z_1=W_1 x^{(i)}+b_1$$
$$a_1=ReLU(z_1)$$
$$z_2=W_1 x'^{(i)}+b_1$$
$$a_2=ReLU(z_2)$$
$$a=a_1-a_2$$
$$z_3=W_2 a+b_2$$
$$\hat{y}^{(i)}=\sigma(z_3)$$
$$L^{(i)}=y^{(i)}\log{\hat{y}}^{(i)}+(1-y^{(i)})\log{(1-\hat{y}^{(i)})}$$
$$J=-\frac{1}{m}\sum_{i=1}^{m}L^{(i)}$$
where inputs are $x^{(i)}\in\mathbb{R}^{d_x\times1}, x'^{(i)}\in\mathbb{R}^{d_x\times 1}$ and the output is $\hat{y}\in(0,1)$ (the label is $y^{(i)}\in\{0, 1\}$). also $a\in\mathbb{R}^{d_a\times 1}$. Compute the following:</p>
<ol>
<li><p>$\frac{\partial J}{\partial z_3}$</p>
</li>
<li><p>$\frac{\partial z_3}{\partial a}$</p>
</li>
<li><p>$\frac{\partial a}{\partial z_1}, and \frac{\partial a}{\partial z_2}$</p>
</li>
<li><p>$\frac{\partial z_2}{\partial W_1}, and \frac{\partial z_1}{\partial W_1}$</p>
</li>
<li><p>$\frac{\partial J}{\partial W_1}$</p>
</li>
<li><p>Write down the formula for updating all weights based on gradient descent.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="CNNs">CNNs<a class="anchor-link" href="#CNNs">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>CNNs are maybe the most important module in image processing. They have some sort of inductive bias for extracting local features.</p>
<p>The source for this topic is:</p>
<ul>
<li>SPML course in Fall 2021 <ul>
<li>session 5 from min 45</li>
<li>session 6</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Questions">Questions<a class="anchor-link" href="#Questions">¶</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Answer the following questions:</p>
<ol>
<li>Describe the <em>Sparsity of Connections</em> property of CNNs.</li>
<li>Describe the <em>Parameter Sharing</em> property of CNNs.</li>
<li>Consider an input of shape $63\times 63\times 16$. If <em>stride=2</em> and <em>padding=0</em>, compute the shape of the output if we have 32 $7\times 7$ kernels.</li>
<li>Name three advantages of using CNN over MLP.</li>
<li>Consider a CNN network, which is trained on ImageNet. Is the output probability of the network uniform overall classes if the input is a white picture? Why?</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Complete the following table. padding and stride are equal to 1, unless explicitly states.</p>
<ul>
<li>CONVx-N is a N filter convolution layer with height and width equal to x.</li>
<li>POOL-N is a MAX Pooling of the shape $N\times N$, with <em>Stride=$N$</em>, and <em>Padding=$0$</em>.</li>
<li>FC-N is a fully-connected layer with N neurons.</li>
</ul>
<table>
<thead><tr>
<th style="text-align:left">Layer</th>
<th style="text-align:right">Output Shape</th>
<th style="text-align:center"># of Parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Input</td>
<td style="text-align:right">128$\times$128$\times$3</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:left">CONV-9-32</td>
<td style="text-align:right"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">POOL-2</td>
<td style="text-align:right"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">CONV-5-64</td>
<td style="text-align:right"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">POOL-2</td>
<td style="text-align:right"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">CONV-5-64</td>
<td style="text-align:right"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">POOL-2</td>
<td style="text-align:right"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">FC-3</td>
<td style="text-align:right"></td>
</tr>
</tbody>
</table>
<p>What is the number of parameters for replacing the fourth layer (CONV-5-64) with a fully-connected layer? What's your conclusion?</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we use a GPU with 12GB RAM for running the following network, what is the maximum number of pictures we could have in a batch? (you should find the memory bottleneck)</p>
<p>Input: 256 x 256</p>
<p>[64] Conv 3 x 3, s=1, p=1</p>
<p>[64] Conv 3 x 3, s=1, p=1</p>
<p>Pool 2 x 2, s=2, p=0</p>
<p>[128] Conv 3 x 3, s=1, p=1</p>
<p>[128] Conv 3 x 3, s=1, p=1</p>
<p>Pool 2 x 2, s=2, p=0</p>
<p>[256] Conv 3 x 3, s=1, p=1</p>
<p>[256] Conv 3 x 3, s=1, p=1</p>
<p>Pool 2 x 2, s=2, p=0</p>
<p>[512] Conv 3 x 3, s=1, p=1</p>
<p>[512] Conv 3 x 3, s=1, p=1</p>
<p>Pool 2 x 2, s=2, p=0</p>
<p>[512] Conv 3 x 3, s=1, p=1</p>
<p>[512] Conv 3 x 3, s=1, p=1
Pool 2 x 2, s=2, p=0</p>
<p>Flatten</p>
<p>FC (4096)</p>
<p>FC (4096)</p>
<p>FC (2)</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Determine the receptive field of the neuron $(i, j)$ in the last convolution layer.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="RNNs">RNNs<a class="anchor-link" href="#RNNs">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Stanford Natural Language Processing with Deep Learning course | Winter 2021<ul>
<li>sessions 5, 6, and 7</li>
</ul>
</li>
<li>AI-Med internship videos<ul>
<li>sessions 6 and 7</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Build a neural network that gets two sequence of binary number and the output is a binary sequence that is a
sum of the two (the length of the sequence is not fixed). For example:</p>
<table>
<thead><tr>
<th>Time</th>
<th>input 1</th>
<th>input 2</th>
<th>output</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>At time 1 the network gets the least significant bit and at time 4 it gets
the most significant bit. Evaluate you NN on large sequences.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Generative-Models">Generative Models<a class="anchor-link" href="#Generative-Models">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Generative models are deep models that generate fake data (image, text, video, etc.). We want that this fake-generated data seem real to humans and even other intelligent systems.
The source for this topic is:</p>
<ul>
<li>AI-Med internship videos<ul>
<li>session 8</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
